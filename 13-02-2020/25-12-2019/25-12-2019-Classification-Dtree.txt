Training a model:

Holdout method:

In case of supervised learning,a model is trained by using the labelled input data.
How can we understand the performance of the model?The test data may not be available immediately.
Also,the label value of the test data is not known..
A part of the input data is held back(thats the name "Holdout" originates).In general 
70-80% of the input data is used for model training and 30-20% is used as test data for the
validation of the performance of the model.

Once the model is trained using the training data,the labels of the test data are predicted using the model's
target function.Then the predicted value is compared with the actual value of the labels.

K-Fold Cross Validation:

Process of repeated holdout is the basis for k-fold cross-validation.
In k-fold cross validation,the data set is divided into k-completely distinct or non-overlapping
random partitions called folds.The value of 'k' can be set to any number acccording to given data.

10-fold cross validation is by far the most popular approach.In this approach,for each of the 10-folds,each 
comprising of appx 10% of the data,one of the folds is used as the test data for validating
model performance trained based on remaining 9 folds.(90% of the data)This is repeated 10 times.
The average performance across all folds is being reported.

Bootstrapping:

In this same data instance may be picked up multiple times in a sample.
since elements are repeated in the sample,possible number of training/test data samples is unlimited.

Classification Steps:

Problem --> Identification of Required data -->Data-Pre Processing -->Definition of training-set
                                   |                    |                             |           
                                                             -->    Algorithm Selection(Model)
							|		 	      |
							     --->		   Training
							|			      | 
                                                                          Evaluation with test set
							|			     |
							Parameter tuning<------ O.K -->Classifier




Decision Tree :

As the name suggests it is a classifier based on a series of logical decisions,which resembles a tree 
with the branches..

A decision tree is used for multi-dimensional analysis with multiple class.
It is characterized by fast
execution time and ease in the implementation of the rules.

Tree terminology :

Root node: The node at the top of the tree.
Child Node : One node is child node of the other if it is below from the node (away from the root node).
Parent Node:One node is parent node of the other if it is above of the node (closer to the root node)
Leaf Node:Node which donot have any child nodes.
Edge :Connection between two nodes.
Degree : Count of subtree of any node.
Height of the node:Number of edges from node to leaf node having the longest edges in the middle.
Height of the tree : Height of the root node.
Siblings : Two nodes are sibling if they share the same parent node.
Splitting : It is a process of dividing a node into two or more sub-nodes
Pruning : When we remove sub-nodes of a decision node,this process is called Pruning.
Depth : It is the length of the path to its root.


A tree is a collection of entities called nodes.

Every node will have its own value or data,and it might or might not possess a child node.
If a rootnode is connected to any other node,that root will become the Parent node and
the connected one will be the child node.

Terminal nodes are the nodes without children,as 
they are the last or end or the bottom nodes of the tree.

Decision Tree Types: 1)Categorical variable Decision tree(categorical target variable) 
2)Continuous Decision tree(continuous target variable)


Popular Decision Tree Algorithms:

1.CART (Classification and Regression Trees) — This makes use of Gini impurity as metric.
2.ID3 (Iterative Dichotomiser 3) — This uses entropy and information gain as metric

Entropy --> Measure of the impurities

Information Gain -->Entropy(S) - Weightedavg * Entropy of each feature..
Entropy   -->  summation (i =1 to c) -plog(base2)p(i)

			where c rep the number of diff class labels and p refers to the proportion of values falling
			into the ith class label.

Gini(E) = 1 - (Σj = 1 to c)p^2 j

Gini impurity tends to isolate the most frequent class in its own branch of the tree,while
entropy tends to produce slightly more balanced trees.


When Italian statistician - and former fascist - Corrado Gini died in Rome on 13 March 1965, 
he could not have known that 50 years on, 
the UN would still use his name in their annual rankings of nations.

It is a way of comparing how distribution of income in a society compares with a similar society 
in which everyone earned exactly the same amount. 
Inequality on the Gini scale is measured between 0, where everybody is equal, and 1, 
where all the country's income is earned by a single person.

Gini Index :It provides an index to measure inequality.
GINI works well with categorical target variable "Success" or "Failure"
It performs only binary splits.

Steps to calculate Gini for a split:
-->Calculate Gini for sub-nodes ,using formula sum of square of probability for success 
and failure (p^2+q^2)    ->p - success,q - failure
-->Calculate Gini for split using weighted Gain score of each node of that split.

E.g: We want to segregate the students based on the target variable (playing Football or not)
     ->Split on Gender  ->Split on Class

-Split on gender:                                      

	Students = 30,Play football = 15
	
	Female ->10 	   and Male->20 
	play football       play football (13)
         (2) ->0.2%(2/10)           ->13/20 ->0.65

->Calculate Gini for sub-node female  = 0.2*0.2(play) + 0.8*0.8(not play) = 0.68
->Gini for sub-node male  = 0.65*0.65 + 0.35*0.35 = 0.55

->Cal weighted gini for Split gender =10/30*0.68+20/30*0.55  --->0.59

Split on class :

          class ix: students = 14,play football = 6(6/14= 0.43)
          class x:  students = 16,play football = 9(9/16 = 0.56)


->Gini for sub-node class ix = 0.43*0.43+0.57*0.57 = 0.51
->Gini for sub-node class x = 0.56*0.56+0.44*0.44 = 0.51
->Cal weighted gini for split class = 14/30 * 0.51 +16/30*0.51  ----> 0.51

From the above we can see the Gini score for split on gender is higher (0.59 >0.51)
than Split on class.Hence node split will
take place on Gender.

Information Gain:

Information gain is created on the basis of the decrease in entropy(S) 
after a dataset is split.Constructing a decision tree
is all about finding an attribute that returns the highest information gain.
Let us consider the same above example:

Steps :
->Calculate entropy of parent node
->Calculate entropy of each individual node of split and calculate weighted average 
of all subnodes available in split.


Students = 30,Play football = 15

Female ->10 	           and Male->20 
	play football      play football (13)
         (2)           


Entropy = -plog(base2)p - qlog(base2)q  
where p and q are the probability of success and failure respectively in that node.

->Entropy for parent node = -15/30log2(15/30) - 15/30log2(15/30) --> 1 [Impure node] it divides 50%
->Entropy for female node = -2/10log2(2/10) -8/10log2(8/10) --> 0.72
->Entropy for male node = -13/20log2(13/20) - 7/20log2(7/20) -->0.93

-->Entropy for split gender = weighted entropy of sub-nodes
                
		-->10/30 * 0.72 + 20/30 * 0.93 -->0.86

->Information gain -> 1 - Entropy = 0.14 (Split on Gender) 

	  class ix: students = 14, play football = 6	
          class x:  students = 16, play football = 9

->Entropy for split on class ix -->-6/14log2(6/14)-8/14log2(8/14) ->0.98
-->Entropy for split on class x -->-9/16log2(9/16) - 7/16log2(7/16) -->0.99

->Entropy for split class --> 14/30 * 0.98 + 16/30 * 0.99 --> 0.99
-->Information Gain --> 1 - 0.99 -->0.01

Split gender --> 0.14 Split Class ---> 0.01

Split on gender is our root node..


Building a Decision tree:

Decision trees are built corresponding to the training data following an approach called 
recursive partitioning.
The approach splits the data into multiple subsets based on the basis of feature values.
->Start with empty decision tree.
->Select best feature to split
->Split the data into different groups based on feature value.
->If no more possibility of split
->Make prediction with majority class so that we get the exact category.
->Split the features recursively.


Main steps are as follows :

-->Begin with your training dataset, which should have some feature variables and 
classification or regression output.

-->Determine the “best feature” in the dataset to split the data on; 
more on how we define “best feature” later [selecting the root node]

-->Split the data into subsets that contain the possible values for this best feature. 

-->This splitting basically defines a node on the tree i.e each node is a splitting point based on a 
certain feature from our data.

-->Recursively generate new tree nodes by using the subset of data created from step 3. 

-->We keep splitting until we reach a point where we have optimised, by some measure, 
maximum accuracy while minimising the number of splits / nodes.


Samples for a node split:It defines the minimum samples required in a node which should be 
considered for splitting.

Samples for a terminal (leaf) node:It defines less samples or observations which are required 
in a terminal node

Maximum depth of a tree:The vertical depth determines the depth of the tree.

Maximum number of terminal nodes:It determines the max number of leaves in a tree.

Maximum features to consider for a split:It determines the number of features to consider while
searching for a perfect split.

The main driver for identifying the the feature is that the data should be split in such a way
that the partitions created by the split should contain examples belonging to a single class.

Difference between Gini or Entropy is first thing is gini impurity is slightly faster to compute and it
isolates the most frequent class to its own branch of the tree,while entropy tends to produce
slightly ,more balanced trees.

Unless a stopping criteria is provided (applied) it may keep growing indefinitely-splitting for 
every feature and dividing into smaller partitions till the point that the data is perfectly classified.

This results in overfitting,where we need to go for Pruning..Pruning a decision tree reduces 
the size of the tree such that the model is more generalized and can classify 
unknown and unlabelled data.

Prepruning : Stop growing the tree before it reaches perfection,once it reaches
certain number of decisions.

Postpruning: Allow the tree to grow entirely and then post-prune some of the branches 
from it.Allow it to grow for full extent..It is the effective approach in terms of 
classification accuracy as it considers all minute information available from the trainng data.

Strengths: For smaller trees,not much mathematical and computational knowledge is required to understand
->Provides a definite clue of which features are more useful for classification.
->It can handle both numerical and categorical variables.

Weakness:Decision tree models are often biased towards features having more number of 
possible values,i.e.,levels.
-->Large trees are complex to understand and it is computationally expensive to train.
 
Balancing overfitting and underfitting in decision tree is a very trickyinvolving more of an art 
than science.The only way to master it is continue working with more number of datasets with lot 
of diversity..





























